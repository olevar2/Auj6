"""
Fibonacci Extension Indicator - Advanced Implementation
=======================================================

This indicator implements sophisticated Fibonacci extension calculations for projecting price targets
beyond the original swing range using Fibonacci ratios. Extensions are crucial for identifying
potential profit targets and reversal zones in trending markets.

Features:
- Advanced multi-swing extension calculations with precise geometric algorithms
- Machine learning extension strength validation and reliability scoring
- Dynamic extension level adjustment based on market volatility and momentum
- Multi-timeframe extension confluence detection and synthesis
- Momentum-based extension probability modeling
- Volume-weighted extension validation
- Advanced support/resistance zone identification
- Comprehensive error handling and edge case management

Mathematical Foundation:
- Primary extensions: 61.8%, 100%, 161.8%, 261.8%, 423.6%
- Custom ratio calculations based on market behavior analysis
- Geometric projection algorithms with volatility adjustment
- Statistical validation using historical extension performance
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Union
from dataclasses import dataclass, field
from sklearn.cluster import DBSCAN
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from scipy import optimize
from scipy.stats import norm
import logging
from datetime import datetime, timedelta

from ..base.standard_indicator import StandardIndicatorInterface, DataRequirement, DataType, SignalType, IndicatorResult
from ....core.exceptions import IndicatorCalculationException

logger = logging.getLogger(__name__)

@dataclass
class FibonacciExtensionLevel:
    """Represents a Fibonacci extension level with comprehensive properties."""
    ratio: float
    price: float
    time_index: int
    strength: float
    confluence_count: int
    volume_confirmation: float
    momentum_alignment: float
    historical_performance: float
    volatility_adjustment: float
    probability_score: float
    projection_quality: float

@dataclass
class SwingPoint:
    """Represents a significant swing point for extension calculations."""
    index: int
    price: float
    time: datetime
    swing_type: str  # 'high' or 'low'
    strength: float
    volume: float
    momentum: float
    validation_score: float

@dataclass
class ExtensionProjection:
    """Represents a complete extension projection from three swing points."""
    swing_low: SwingPoint
    swing_high: SwingPoint
    swing_retracement: SwingPoint
    extension_levels: List[FibonacciExtensionLevel]
    projection_direction: str  # 'bullish' or 'bearish'
    overall_strength: float
    confidence: float
    target_zone: Tuple[float, float]
    time_projection: Optional[datetime]

@dataclass
class ExtensionCluster:
    """Represents a cluster of nearby extension levels for confluence analysis."""
    levels: List[FibonacciExtensionLevel] = field(default_factory=list)
    center_price: float = 0.0
    strength: float = 0.0
    confluence_score: float = 0.0
    support_resistance_type: str = ""  # 'support', 'resistance', or 'neutral'
    time_window: Tuple[datetime, datetime] = field(default_factory=lambda: (datetime.now(), datetime.now()))

class FibonacciExtensionIndicator(StandardIndicatorInterface):
    """
    Advanced Fibonacci Extension Indicator
    
    Calculates multiple extension projections from significant swing patterns and
    provides sophisticated target identification with machine learning validation.
    """
    
    def __init__(self, 
                 lookback_period: int = 200,
                 min_swing_strength: float = 0.3,
                 extension_ratios: Optional[List[float]] = None,
                 volatility_adjustment: bool = True,
                 ml_validation: bool = True,
                 confluence_threshold: float = 0.02,
                 min_projection_distance: float = 0.01):
        """
        Initialize the Fibonacci Extension Indicator
        
        Args:
            lookback_period: Period for swing point identification
            min_swing_strength: Minimum strength for swing point validation
            extension_ratios: Custom Fibonacci ratios (default: standard ratios)
            volatility_adjustment: Enable volatility-based adjustments
            ml_validation: Enable machine learning validation
            confluence_threshold: Price proximity threshold for confluence
            min_projection_distance: Minimum distance for valid projections
        """
        super().__init__()
        self.lookback_period = lookback_period
        self.min_swing_strength = min_swing_strength
        self.extension_ratios = extension_ratios or [0.618, 1.0, 1.618, 2.618, 4.236]
        self.volatility_adjustment = volatility_adjustment
        self.ml_validation = ml_validation
        self.confluence_threshold = confluence_threshold
        self.min_projection_distance = min_projection_distance
        
        # Machine learning components
        self.extension_validator = None
        self.historical_performance = {}
        self.scaler = StandardScaler()
        
        # Analysis state
        self.swing_points_cache = []
        self.projections_cache = []
        self.performance_tracker = {}
        
        logger.info(f"FibonacciExtensionIndicator initialized with {len(self.extension_ratios)} ratios")

    def get_data_requirements(self) -> List[DataRequirement]:
        """Define the data requirements for this indicator."""
        return [
            DataRequirement(
                data_type=DataType.OHLCV,
                required_columns=['open', 'high', 'low', 'close', 'volume'],
                min_periods=max(self.lookback_period, 50),
                description="OHLCV data for swing analysis and extension calculations"
            )
        ]

    def calculate_raw(self, data: pd.DataFrame) -> Dict[str, Any]:
        """
        Calculate Fibonacci extension levels with advanced algorithms.
        
        Args:
            data: OHLCV DataFrame with required columns
            
        Returns:
            Dictionary containing extension calculations and analysis
        """
        try:
            if data.empty or len(data) < self.lookback_period:
                raise IndicatorCalculationException("Insufficient data for extension calculations")
            
            # Validate required columns
            required_cols = ['open', 'high', 'low', 'close', 'volume']
            missing_cols = [col for col in required_cols if col not in data.columns]
            if missing_cols:
                raise IndicatorCalculationException(f"Missing required columns: {missing_cols}")
            
            # Initialize calculation components
            self._initialize_ml_models(data)
            
            # Step 1: Identify significant swing points
            swing_points = self._identify_swing_points(data)
            
            # Step 2: Generate extension projections
            projections = self._calculate_extension_projections(data, swing_points)
            
            # Step 3: Analyze confluence zones
            confluence_zones = self._analyze_confluence_zones(projections)
            
            # Step 4: Calculate current market position relative to extensions
            current_analysis = self._analyze_current_position(data, projections, confluence_zones)
            
            # Step 5: Generate quality scores and rankings
            quality_metrics = self._calculate_quality_metrics(projections, confluence_zones)
            
            # Step 6: Perform machine learning validation
            if self.ml_validation:
                ml_scores = self._perform_ml_validation(data, projections)
            else:
                ml_scores = {}
            
            return {
                'swing_points': swing_points,
                'projections': projections,
                'confluence_zones': confluence_zones,
                'current_analysis': current_analysis,
                'quality_metrics': quality_metrics,
                'ml_validation': ml_scores,
                'metadata': {
                    'calculation_time': datetime.now(),
                    'data_points_analyzed': len(data),
                    'swing_points_found': len(swing_points),
                    'projections_generated': len(projections),
                    'confluence_zones_identified': len(confluence_zones)
                }
            }
            
        except Exception as e:
            logger.error(f"Error in Fibonacci extension calculation: {str(e)}")
            raise IndicatorCalculationException(f"Extension calculation failed: {str(e)}")

    def _initialize_ml_models(self, data: pd.DataFrame) -> None:
        """Initialize machine learning models for extension validation."""
        try:
            if not self.ml_validation:
                return
            
            # Prepare features for extension quality prediction
            features = self._extract_ml_features(data)
            
            if len(features) > 50:  # Minimum data for meaningful ML
                # Initialize Random Forest for extension validation
                self.extension_validator = RandomForestRegressor(
                    n_estimators=100,
                    max_depth=10,
                    min_samples_split=5,
                    random_state=42
                )
                
                # Train on historical extension performance if available
                if len(self.historical_performance) > 20:
                    self._train_extension_validator(features)
            
        except Exception as e:
            logger.warning(f"ML initialization failed: {str(e)}")
            self.ml_validation = False

    def _extract_ml_features(self, data: pd.DataFrame) -> np.ndarray:
        """Extract features for machine learning validation."""
        try:
            # Technical features
            returns = data['close'].pct_change()
            volatility = returns.rolling(20).std()
            volume_sma = data['volume'].rolling(20).mean()
            price_position = (data['close'] - data['close'].rolling(50).min()) / \
                           (data['close'].rolling(50).max() - data['close'].rolling(50).min())
            
            # Momentum features
            rsi = self._calculate_rsi(data['close'])
            macd = self._calculate_macd(data['close'])
            
            # Volume features
            volume_ratio = data['volume'] / volume_sma
            price_volume_trend = self._calculate_pvt(data)
            
            features = np.column_stack([
                volatility.fillna(0),
                price_position.fillna(0.5),
                rsi.fillna(50),
                macd.fillna(0),
                volume_ratio.fillna(1),
                price_volume_trend.fillna(0)
            ])
            
            return features
            
        except Exception as e:
            logger.error(f"Feature extraction failed: {str(e)}")
            return np.array([])

    def _identify_swing_points(self, data: pd.DataFrame) -> List[SwingPoint]:
        """Identify significant swing highs and lows with advanced validation."""
        try:
            swing_points = []
            
            # Parameters for swing detection
            min_periods = 10
            strength_lookback = 20
            
            for i in range(min_periods, len(data) - min_periods):
                current_high = data['high'].iloc[i]
                current_low = data['low'].iloc[i]
                current_volume = data['volume'].iloc[i]
                
                # Check for swing high
                if self._is_swing_high(data, i, min_periods):
                    strength = self._calculate_swing_strength(data, i, 'high', strength_lookback)
                    if strength >= self.min_swing_strength:
                        momentum = self._calculate_momentum_at_point(data, i)
                        validation = self._validate_swing_point(data, i, 'high')
                        
                        swing_points.append(SwingPoint(
                            index=i,
                            price=current_high,
                            time=data.index[i] if hasattr(data.index, 'to_pydatetime') else datetime.now(),
                            swing_type='high',
                            strength=strength,
                            volume=current_volume,
                            momentum=momentum,
                            validation_score=validation
                        ))
                
                # Check for swing low
                if self._is_swing_low(data, i, min_periods):
                    strength = self._calculate_swing_strength(data, i, 'low', strength_lookback)
                    if strength >= self.min_swing_strength:
                        momentum = self._calculate_momentum_at_point(data, i)
                        validation = self._validate_swing_point(data, i, 'low')
                        
                        swing_points.append(SwingPoint(
                            index=i,
                            price=current_low,
                            time=data.index[i] if hasattr(data.index, 'to_pydatetime') else datetime.now(),
                            swing_type='low',
                            strength=strength,
                            volume=current_volume,
                            momentum=momentum,
                            validation_score=validation
                        ))
            
            # Sort by index and filter for quality
            swing_points.sort(key=lambda x: x.index)
            filtered_points = [sp for sp in swing_points if sp.validation_score > 0.5]
            
            logger.info(f"Identified {len(filtered_points)} validated swing points")
            return filtered_points
            
        except Exception as e:
            logger.error(f"Swing point identification failed: {str(e)}")
            return []

    def _is_swing_high(self, data: pd.DataFrame, index: int, periods: int) -> bool:
        """Check if the point is a swing high."""
        try:
            current_high = data['high'].iloc[index]
            left_highs = data['high'].iloc[index-periods:index]
            right_highs = data['high'].iloc[index+1:index+periods+1]
            
            return (current_high > left_highs.max()) and (current_high > right_highs.max())
        except:
            return False

    def _is_swing_low(self, data: pd.DataFrame, index: int, periods: int) -> bool:
        """Check if the point is a swing low."""
        try:
            current_low = data['low'].iloc[index]
            left_lows = data['low'].iloc[index-periods:index]
            right_lows = data['low'].iloc[index+1:index+periods+1]
            
            return (current_low < left_lows.min()) and (current_low < right_lows.min())
        except:
            return False

    def _calculate_swing_strength(self, data: pd.DataFrame, index: int, 
                                swing_type: str, lookback: int) -> float:
        """Calculate the strength of a swing point."""
        try:
            if swing_type == 'high':
                current_price = data['high'].iloc[index]
                nearby_highs = data['high'].iloc[max(0, index-lookback):index+lookback+1]
                percentile = (nearby_highs < current_price).sum() / len(nearby_highs)
            else:
                current_price = data['low'].iloc[index]
                nearby_lows = data['low'].iloc[max(0, index-lookback):index+lookback+1]
                percentile = (nearby_lows > current_price).sum() / len(nearby_lows)
            
            # Volume confirmation
            avg_volume = data['volume'].iloc[max(0, index-lookback):index+lookback+1].mean()
            current_volume = data['volume'].iloc[index]
            volume_factor = min(2.0, current_volume / avg_volume) if avg_volume > 0 else 1.0
            
            return min(1.0, percentile * volume_factor)
            
        except Exception as e:
            logger.error(f"Swing strength calculation failed: {str(e)}")
            return 0.0

    def _calculate_momentum_at_point(self, data: pd.DataFrame, index: int) -> float:
        """Calculate momentum at a specific point."""
        try:
            if index < 14:
                return 0.0
            
            # ROC momentum
            roc = (data['close'].iloc[index] - data['close'].iloc[index-14]) / data['close'].iloc[index-14]
            
            # RSI momentum
            rsi = self._calculate_rsi(data['close'].iloc[:index+1]).iloc[-1]
            rsi_momentum = (rsi - 50) / 50
            
            # Combined momentum
            return (roc + rsi_momentum) / 2
            
        except Exception as e:
            logger.error(f"Momentum calculation failed: {str(e)}")
            return 0.0

    def _validate_swing_point(self, data: pd.DataFrame, index: int, swing_type: str) -> float:
        """Validate swing point using multiple criteria."""
        try:
            validation_score = 0.0
            
            # Price action validation
            if swing_type == 'high':
                if index > 0 and data['close'].iloc[index] < data['open'].iloc[index]:
                    validation_score += 0.3  # Bearish reversal candle
            else:
                if index > 0 and data['close'].iloc[index] > data['open'].iloc[index]:
                    validation_score += 0.3  # Bullish reversal candle
            
            # Volume validation
            if index >= 5:
                avg_volume = data['volume'].iloc[index-5:index].mean()
                current_volume = data['volume'].iloc[index]
                if current_volume > avg_volume:
                    validation_score += 0.4
            
            # Follow-through validation
            if index < len(data) - 3:
                if swing_type == 'high':
                    follow_through = data['close'].iloc[index+1:index+4].max() < data['high'].iloc[index]
                else:
                    follow_through = data['close'].iloc[index+1:index+4].min() > data['low'].iloc[index]
                
                if follow_through:
                    validation_score += 0.3
            
            return min(1.0, validation_score)
            
        except Exception as e:
            logger.error(f"Swing validation failed: {str(e)}")
            return 0.0

    def _calculate_extension_projections(self, data: pd.DataFrame, 
                                       swing_points: List[SwingPoint]) -> List[ExtensionProjection]:
        """Calculate Fibonacci extension projections from swing patterns."""
        try:
            projections = []
            
            # Find ABC patterns for extension calculations
            for i in range(len(swing_points) - 2):
                for j in range(i + 1, len(swing_points) - 1):
                    for k in range(j + 1, len(swing_points)):
                        
                        swing_a = swing_points[i]
                        swing_b = swing_points[j]
                        swing_c = swing_points[k]
                        
                        # Validate ABC pattern
                        if self._is_valid_abc_pattern(swing_a, swing_b, swing_c):
                            projection = self._calculate_abc_extensions(data, swing_a, swing_b, swing_c)
                            if projection:
                                projections.append(projection)
            
            # Sort by overall strength
            projections.sort(key=lambda x: x.overall_strength, reverse=True)
            
            # Limit to top projections to avoid noise
            max_projections = 20
            return projections[:max_projections]
            
        except Exception as e:
            logger.error(f"Extension projection calculation failed: {str(e)}")
            return []

    def _is_valid_abc_pattern(self, swing_a: SwingPoint, swing_b: SwingPoint, 
                            swing_c: SwingPoint) -> bool:
        """Validate if three swings form a valid ABC pattern for extensions."""
        try:
            # Check chronological order
            if not (swing_a.index < swing_b.index < swing_c.index):
                return False
            
            # Check alternating swing types for classic ABC
            if swing_a.swing_type == swing_c.swing_type:
                return False
            
            # Check minimum retracement
            if swing_a.swing_type == 'low' and swing_c.swing_type == 'low':
                # Bullish pattern: Low-High-Low
                range_ab = swing_b.price - swing_a.price
                range_bc = swing_b.price - swing_c.price
                retracement_ratio = range_bc / range_ab if range_ab != 0 else 0
                return 0.236 <= retracement_ratio <= 0.786
            
            elif swing_a.swing_type == 'high' and swing_c.swing_type == 'high':
                # Bearish pattern: High-Low-High
                range_ab = swing_a.price - swing_b.price
                range_bc = swing_c.price - swing_b.price
                retracement_ratio = range_bc / range_ab if range_ab != 0 else 0
                return 0.236 <= retracement_ratio <= 0.786
            
            return False
            
        except Exception as e:
            logger.error(f"ABC pattern validation failed: {str(e)}")
            return False

    def _calculate_abc_extensions(self, data: pd.DataFrame, swing_a: SwingPoint, 
                                swing_b: SwingPoint, swing_c: SwingPoint) -> Optional[ExtensionProjection]:
        """Calculate Fibonacci extensions from ABC pattern."""
        try:
            # Determine projection direction
            if swing_a.swing_type == 'low':
                # Bullish extension (Low-High-Low pattern)
                projection_direction = 'bullish'
                base_price = swing_c.price
                range_ab = swing_b.price - swing_a.price
                direction_multiplier = 1
            else:
                # Bearish extension (High-Low-High pattern)
                projection_direction = 'bearish'
                base_price = swing_c.price
                range_ab = swing_a.price - swing_b.price
                direction_multiplier = -1
            
            # Calculate extension levels
            extension_levels = []
            for ratio in self.extension_ratios:
                extension_price = base_price + (direction_multiplier * range_ab * ratio)
                
                # Validate extension distance
                distance_pct = abs(extension_price - base_price) / base_price
                if distance_pct < self.min_projection_distance:
                    continue
                
                # Calculate level properties
                strength = self._calculate_extension_strength(swing_a, swing_b, swing_c, ratio)
                confluence = self._count_confluence_at_level(data, extension_price)
                volume_confirmation = self._get_volume_confirmation(data, swing_c.index)
                
                # Apply volatility adjustment if enabled
                if self.volatility_adjustment:
                    volatility_adj = self._calculate_volatility_adjustment(data, swing_c.index)
                    extension_price *= (1 + volatility_adj * 0.1)  # Small volatility adjustment
                
                level = FibonacciExtensionLevel(
                    ratio=ratio,
                    price=extension_price,
                    time_index=swing_c.index,
                    strength=strength,
                    confluence_count=confluence,
                    volume_confirmation=volume_confirmation,
                    momentum_alignment=self._calculate_momentum_alignment(data, swing_c.index, projection_direction),
                    historical_performance=self._get_historical_performance(ratio),
                    volatility_adjustment=volatility_adj if self.volatility_adjustment else 0.0,
                    probability_score=self._calculate_probability_score(strength, confluence, volume_confirmation),
                    projection_quality=self._assess_projection_quality(swing_a, swing_b, swing_c)
                )
                
                extension_levels.append(level)
            
            if not extension_levels:
                return None
            
            # Calculate overall projection metrics
            overall_strength = np.mean([level.strength for level in extension_levels])
            confidence = self._calculate_projection_confidence(swing_a, swing_b, swing_c, extension_levels)
            
            # Identify primary target zone
            primary_levels = [level for level in extension_levels if level.ratio in [1.0, 1.618]]
            if primary_levels:
                target_zone = (
                    min(level.price for level in primary_levels),
                    max(level.price for level in primary_levels)
                )
            else:
                target_zone = (extension_levels[0].price, extension_levels[-1].price)
            
            # Time projection (optional advanced feature)
            time_projection = self._calculate_time_projection(swing_a, swing_b, swing_c)
            
            return ExtensionProjection(
                swing_low=swing_a if swing_a.swing_type == 'low' else swing_b,
                swing_high=swing_b if swing_b.swing_type == 'high' else swing_a,
                swing_retracement=swing_c,
                extension_levels=extension_levels,
                projection_direction=projection_direction,
                overall_strength=overall_strength,
                confidence=confidence,
                target_zone=target_zone,
                time_projection=time_projection
            )
            
        except Exception as e:
            logger.error(f"ABC extension calculation failed: {str(e)}")
            return None

    def _calculate_extension_strength(self, swing_a: SwingPoint, swing_b: SwingPoint, 
                                    swing_c: SwingPoint, ratio: float) -> float:
        """Calculate the strength of an extension level."""
        try:
            strength = 0.0
            
            # Base strength from swing point qualities
            swing_strength = (swing_a.strength + swing_b.strength + swing_c.strength) / 3
            strength += swing_strength * 0.4
            
            # Ratio-based strength (standard Fibonacci ratios are stronger)
            standard_ratios = [0.618, 1.0, 1.618, 2.618]
            if ratio in standard_ratios:
                strength += 0.3
            else:
                strength += 0.1
            
            # Pattern quality
            range_ab = abs(swing_b.price - swing_a.price)
            range_bc = abs(swing_c.price - swing_b.price)
            if range_ab > 0:
                retracement_ratio = range_bc / range_ab
                # Optimal retracement range
                if 0.38 <= retracement_ratio <= 0.618:
                    strength += 0.3
                elif 0.236 <= retracement_ratio <= 0.786:
                    strength += 0.2
            
            return min(1.0, strength)
            
        except Exception as e:
            logger.error(f"Extension strength calculation failed: {str(e)}")
            return 0.0

    def _count_confluence_at_level(self, data: pd.DataFrame, target_price: float) -> int:
        """Count confluence factors at a specific price level."""
        try:
            confluence_count = 0
            tolerance = target_price * self.confluence_threshold
            
            # Check for historical support/resistance
            for i in range(len(data)):
                if abs(data['high'].iloc[i] - target_price) <= tolerance:
                    confluence_count += 1
                if abs(data['low'].iloc[i] - target_price) <= tolerance:
                    confluence_count += 1
            
            # Check for moving average confluence
            if len(data) >= 200:
                ma200 = data['close'].rolling(200).mean().iloc[-1]
                if abs(ma200 - target_price) <= tolerance:
                    confluence_count += 2
            
            if len(data) >= 50:
                ma50 = data['close'].rolling(50).mean().iloc[-1]
                if abs(ma50 - target_price) <= tolerance:
                    confluence_count += 1
            
            return confluence_count
            
        except Exception as e:
            logger.error(f"Confluence counting failed: {str(e)}")
            return 0

    def _get_volume_confirmation(self, data: pd.DataFrame, index: int) -> float:
        """Get volume confirmation for extension validity."""
        try:
            if index < 20:
                return 0.5
            
            current_volume = data['volume'].iloc[index]
            avg_volume = data['volume'].iloc[index-20:index].mean()
            
            volume_ratio = current_volume / avg_volume if avg_volume > 0 else 1.0
            return min(1.0, volume_ratio / 2.0)
            
        except Exception as e:
            logger.error(f"Volume confirmation calculation failed: {str(e)}")
            return 0.5

    def _calculate_momentum_alignment(self, data: pd.DataFrame, index: int, direction: str) -> float:
        """Calculate momentum alignment with extension direction."""
        try:
            if index < 14:
                return 0.5
            
            # RSI momentum
            rsi = self._calculate_rsi(data['close'].iloc[:index+1]).iloc[-1]
            
            if direction == 'bullish':
                # For bullish extensions, higher RSI is better (but not overbought)
                if 50 <= rsi <= 70:
                    momentum_score = (rsi - 50) / 20
                elif rsi > 70:
                    momentum_score = 0.3  # Overbought concern
                else:
                    momentum_score = 0.1  # Weak momentum
            else:
                # For bearish extensions, lower RSI is better (but not oversold)
                if 30 <= rsi <= 50:
                    momentum_score = (50 - rsi) / 20
                elif rsi < 30:
                    momentum_score = 0.3  # Oversold concern
                else:
                    momentum_score = 0.1  # Weak momentum
            
            return min(1.0, momentum_score)
            
        except Exception as e:
            logger.error(f"Momentum alignment calculation failed: {str(e)}")
            return 0.5

    def _get_historical_performance(self, ratio: float) -> float:
        """Get historical performance score for a specific ratio."""
        try:
            if ratio in self.historical_performance:
                return self.historical_performance[ratio]
            
            # Default performance based on common market behavior
            performance_map = {
                0.618: 0.7,
                1.0: 0.8,
                1.618: 0.9,
                2.618: 0.6,
                4.236: 0.4
            }
            
            return performance_map.get(ratio, 0.5)
            
        except:
            return 0.5

    def _calculate_probability_score(self, strength: float, confluence: int, 
                                   volume_confirmation: float) -> float:
        """Calculate probability score for extension level achievement."""
        try:
            # Weighted combination of factors
            base_probability = strength * 0.4
            confluence_factor = min(1.0, confluence / 5.0) * 0.3
            volume_factor = volume_confirmation * 0.3
            
            return min(1.0, base_probability + confluence_factor + volume_factor)
            
        except:
            return 0.5

    def _assess_projection_quality(self, swing_a: SwingPoint, swing_b: SwingPoint, 
                                 swing_c: SwingPoint) -> float:
        """Assess overall quality of the ABC projection."""
        try:
            quality_score = 0.0
            
            # Time spacing quality
            time_ab = swing_b.index - swing_a.index
            time_bc = swing_c.index - swing_b.index
            if time_ab > 0:
                time_ratio = time_bc / time_ab
                if 0.5 <= time_ratio <= 2.0:
                    quality_score += 0.3
            
            # Price range quality
            range_ab = abs(swing_b.price - swing_a.price)
            range_bc = abs(swing_c.price - swing_b.price)
            if range_ab > 0:
                range_ratio = range_bc / range_ab
                if 0.236 <= range_ratio <= 0.786:
                    quality_score += 0.4
            
            # Validation scores
            avg_validation = (swing_a.validation_score + swing_b.validation_score + swing_c.validation_score) / 3
            quality_score += avg_validation * 0.3
            
            return min(1.0, quality_score)
            
        except Exception as e:
            logger.error(f"Projection quality assessment failed: {str(e)}")
            return 0.5

    def _calculate_time_projection(self, swing_a: SwingPoint, swing_b: SwingPoint, 
                                 swing_c: SwingPoint) -> Optional[datetime]:
        """Calculate time projection for extension completion."""
        try:
            # Simple time projection based on historical patterns
            time_ab = swing_b.index - swing_a.index
            time_bc = swing_c.index - swing_b.index
            
            # Project extension time as average of AB and BC periods
            if time_ab > 0 and time_bc > 0:
                projected_periods = int((time_ab + time_bc) / 2)
                if hasattr(swing_c, 'time') and isinstance(swing_c.time, datetime):
                    return swing_c.time + timedelta(days=projected_periods)
            
            return None
            
        except Exception as e:
            logger.error(f"Time projection calculation failed: {str(e)}")
            return None

    def _calculate_projection_confidence(self, swing_a: SwingPoint, swing_b: SwingPoint, 
                                       swing_c: SwingPoint, levels: List[FibonacciExtensionLevel]) -> float:
        """Calculate overall confidence in the projection."""
        try:
            if not levels:
                return 0.0
            
            # Average level confidence
            avg_probability = np.mean([level.probability_score for level in levels])
            
            # Pattern strength
            pattern_strength = (swing_a.strength + swing_b.strength + swing_c.strength) / 3
            
            # Quality factors
            quality_factor = levels[0].projection_quality  # Assuming all levels have same quality
            
            # Volume confirmation
            avg_volume_confirmation = np.mean([level.volume_confirmation for level in levels])
            
            # Weighted confidence
            confidence = (
                avg_probability * 0.3 +
                pattern_strength * 0.25 +
                quality_factor * 0.25 +
                avg_volume_confirmation * 0.2
            )
            
            return min(1.0, confidence)
            
        except Exception as e:
            logger.error(f"Projection confidence calculation failed: {str(e)}")
            return 0.0

    def _analyze_confluence_zones(self, projections: List[ExtensionProjection]) -> List[ExtensionCluster]:
        """Analyze confluence zones where multiple extensions converge."""
        try:
            if not projections:
                return []
            
            # Collect all extension levels
            all_levels = []
            for projection in projections:
                all_levels.extend(projection.extension_levels)
            
            if not all_levels:
                return []
            
            # Prepare data for clustering
            price_data = np.array([[level.price] for level in all_levels])
            
            # Use DBSCAN for confluence detection
            clustering = DBSCAN(
                eps=price_data.std() * self.confluence_threshold * 2,
                min_samples=2
            ).fit(price_data)
            
            # Group levels by clusters
            clusters = []
            unique_labels = set(clustering.labels_)
            
            for label in unique_labels:
                if label == -1:  # Noise points
                    continue
                
                cluster_indices = np.where(clustering.labels_ == label)[0]
                cluster_levels = [all_levels[i] for i in cluster_indices]
                
                if len(cluster_levels) >= 2:
                    # Calculate cluster properties
                    center_price = np.mean([level.price for level in cluster_levels])
                    total_strength = sum(level.strength for level in cluster_levels)
                    confluence_score = len(cluster_levels) * total_strength
                    
                    # Determine support/resistance type
                    current_price = price_data[-1][0]  # Assume last price is current
                    if center_price > current_price:
                        sr_type = 'resistance'
                    elif center_price < current_price:
                        sr_type = 'support'
                    else:
                        sr_type = 'neutral'
                    
                    # Time window calculation
                    min_time = min(level.time_index for level in cluster_levels)
                    max_time = max(level.time_index for level in cluster_levels)
                    
                    cluster = ExtensionCluster(
                        levels=cluster_levels,
                        center_price=center_price,
                        strength=total_strength,
                        confluence_score=confluence_score,
                        support_resistance_type=sr_type,
                        time_window=(datetime.now(), datetime.now())  # Simplified for now
                    )
                    
                    clusters.append(cluster)
            
            # Sort by confluence score
            clusters.sort(key=lambda x: x.confluence_score, reverse=True)
            
            return clusters
            
        except Exception as e:
            logger.error(f"Confluence zone analysis failed: {str(e)}")
            return []

    def _analyze_current_position(self, data: pd.DataFrame, projections: List[ExtensionProjection],
                                confluence_zones: List[ExtensionCluster]) -> Dict[str, Any]:
        """Analyze current market position relative to extension levels."""
        try:
            if data.empty:
                return {}
            
            current_price = data['close'].iloc[-1]
            
            analysis = {
                'current_price': current_price,
                'nearest_extensions': [],
                'confluence_proximity': [],
                'projection_status': {},
                'market_phase': 'unknown'
            }
            
            # Find nearest extension levels
            all_levels = []
            for projection in projections:
                all_levels.extend(projection.extension_levels)
            
            if all_levels:
                # Sort by distance from current price
                levels_with_distance = [
                    (level, abs(level.price - current_price))
                    for level in all_levels
                ]
                levels_with_distance.sort(key=lambda x: x[1])
                
                analysis['nearest_extensions'] = [
                    {
                        'ratio': level.ratio,
                        'price': level.price,
                        'distance_pct': distance / current_price * 100,
                        'strength': level.strength,
                        'probability': level.probability_score
                    }
                    for level, distance in levels_with_distance[:5]  # Top 5 nearest
                ]
            
            # Confluence zone proximity
            for cluster in confluence_zones:
                distance_pct = abs(cluster.center_price - current_price) / current_price * 100
                if distance_pct <= 5.0:  # Within 5%
                    analysis['confluence_proximity'].append({
                        'center_price': cluster.center_price,
                        'distance_pct': distance_pct,
                        'strength': cluster.strength,
                        'type': cluster.support_resistance_type,
                        'level_count': len(cluster.levels)
                    })
            
            # Projection status analysis
            bullish_projections = [p for p in projections if p.projection_direction == 'bullish']
            bearish_projections = [p for p in projections if p.projection_direction == 'bearish']
            
            analysis['projection_status'] = {
                'total_projections': len(projections),
                'bullish_count': len(bullish_projections),
                'bearish_count': len(bearish_projections),
                'avg_bullish_confidence': np.mean([p.confidence for p in bullish_projections]) if bullish_projections else 0,
                'avg_bearish_confidence': np.mean([p.confidence for p in bearish_projections]) if bearish_projections else 0
            }
            
            # Market phase determination
            if len(bullish_projections) > len(bearish_projections):
                if analysis['projection_status']['avg_bullish_confidence'] > 0.6:
                    analysis['market_phase'] = 'bullish_extension'
                else:
                    analysis['market_phase'] = 'uncertain_bullish'
            elif len(bearish_projections) > len(bullish_projections):
                if analysis['projection_status']['avg_bearish_confidence'] > 0.6:
                    analysis['market_phase'] = 'bearish_extension'
                else:
                    analysis['market_phase'] = 'uncertain_bearish'
            else:
                analysis['market_phase'] = 'neutral'
            
            return analysis
            
        except Exception as e:
            logger.error(f"Current position analysis failed: {str(e)}")
            return {}

    def _calculate_quality_metrics(self, projections: List[ExtensionProjection],
                                 confluence_zones: List[ExtensionCluster]) -> Dict[str, Any]:
        """Calculate overall quality metrics for the extension analysis."""
        try:
            if not projections:
                return {'overall_quality': 0.0}
            
            # Projection quality metrics
            avg_confidence = np.mean([p.confidence for p in projections])
            avg_strength = np.mean([p.overall_strength for p in projections])
            
            # Level quality metrics
            all_levels = []
            for projection in projections:
                all_levels.extend(projection.extension_levels)
            
            if all_levels:
                avg_probability = np.mean([level.probability_score for level in all_levels])
                avg_confluence = np.mean([level.confluence_count for level in all_levels])
            else:
                avg_probability = 0.0
                avg_confluence = 0.0
            
            # Confluence quality
            confluence_quality = 0.0
            if confluence_zones:
                confluence_quality = np.mean([zone.confluence_score for zone in confluence_zones])
                confluence_quality = min(1.0, confluence_quality / 10.0)  # Normalize
            
            # Overall quality score
            overall_quality = (
                avg_confidence * 0.3 +
                avg_strength * 0.25 +
                avg_probability * 0.25 +
                confluence_quality * 0.2
            )
            
            return {
                'overall_quality': overall_quality,
                'avg_projection_confidence': avg_confidence,
                'avg_projection_strength': avg_strength,
                'avg_level_probability': avg_probability,
                'avg_confluence_count': avg_confluence,
                'confluence_zones_quality': confluence_quality,
                'total_projections': len(projections),
                'total_levels': len(all_levels),
                'total_confluence_zones': len(confluence_zones)
            }
            
        except Exception as e:
            logger.error(f"Quality metrics calculation failed: {str(e)}")
            return {'overall_quality': 0.0}

    def _perform_ml_validation(self, data: pd.DataFrame, 
                             projections: List[ExtensionProjection]) -> Dict[str, Any]:
        """Perform machine learning validation of extension projections."""
        try:
            if not self.ml_validation or not self.extension_validator:
                return {'ml_validation_available': False}
            
            validation_results = {
                'ml_validation_available': True,
                'projection_scores': [],
                'confidence_adjustments': [],
                'feature_importance': {}
            }
            
            # Extract current features
            features = self._extract_ml_features(data)
            if len(features) == 0:
                return {'ml_validation_available': False}
            
            current_features = features[-1:].reshape(1, -1)
            
            # Validate each projection
            for i, projection in enumerate(projections):
                try:
                    # Predict extension success probability
                    ml_score = self.extension_validator.predict(current_features)[0]
                    ml_score = max(0.0, min(1.0, ml_score))  # Clamp to [0, 1]
                    
                    validation_results['projection_scores'].append({
                        'projection_index': i,
                        'ml_score': ml_score,
                        'original_confidence': projection.confidence,
                        'direction': projection.projection_direction
                    })
                    
                    # Calculate confidence adjustment
                    adjustment = (ml_score - 0.5) * 0.2  # Max Â±10% adjustment
                    validation_results['confidence_adjustments'].append(adjustment)
                    
                except Exception as e:
                    logger.warning(f"ML validation failed for projection {i}: {str(e)}")
                    validation_results['projection_scores'].append({
                        'projection_index': i,
                        'ml_score': 0.5,
                        'original_confidence': projection.confidence,
                        'direction': projection.projection_direction
                    })
                    validation_results['confidence_adjustments'].append(0.0)
            
            # Feature importance (if available)
            if hasattr(self.extension_validator, 'feature_importances_'):
                feature_names = ['volatility', 'price_position', 'rsi', 'macd', 'volume_ratio', 'pvt']
                importance_dict = dict(zip(feature_names, self.extension_validator.feature_importances_))
                validation_results['feature_importance'] = importance_dict
            
            return validation_results
            
        except Exception as e:
            logger.error(f"ML validation failed: {str(e)}")
            return {'ml_validation_available': False}

    def _train_extension_validator(self, features: np.ndarray) -> None:
        """Train the extension validator model with historical data."""
        try:
            # This is a simplified training process
            # In production, you would use actual historical extension outcomes
            
            # Create synthetic training data based on historical performance
            # This would be replaced with real historical extension success/failure data
            n_samples = min(len(features), 100)
            X_train = features[-n_samples:]
            
            # Synthetic target based on feature quality
            # This would be replaced with actual historical extension achievement rates
            y_train = np.random.random(n_samples)  # Placeholder
            
            # Scale features
            X_train_scaled = self.scaler.fit_transform(X_train)
            
            # Train model
            self.extension_validator.fit(X_train_scaled, y_train)
            
            logger.info("Extension validator model trained successfully")
            
        except Exception as e:
            logger.error(f"Extension validator training failed: {str(e)}")
            self.ml_validation = False

    def _calculate_volatility_adjustment(self, data: pd.DataFrame, index: int) -> float:
        """Calculate volatility adjustment factor."""
        try:
            if index < 20:
                return 0.0
            
            # Calculate recent volatility
            returns = data['close'].pct_change().iloc[index-20:index]
            current_volatility = returns.std()
            
            # Calculate long-term volatility
            if index >= 60:
                long_term_returns = data['close'].pct_change().iloc[index-60:index-20]
                long_term_volatility = long_term_returns.std()
            else:
                long_term_volatility = current_volatility
            
            # Volatility ratio
            if long_term_volatility > 0:
                volatility_ratio = current_volatility / long_term_volatility
                # Adjustment: higher volatility = wider targets
                return (volatility_ratio - 1.0) * 0.1  # Max Â±10% adjustment
            
            return 0.0
            
        except Exception as e:
            logger.error(f"Volatility adjustment calculation failed: {str(e)}")
            return 0.0

    def _calculate_rsi(self, prices: pd.Series, period: int = 14) -> pd.Series:
        """Calculate RSI indicator."""
        try:
            delta = prices.diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
            rs = gain / loss
            rsi = 100 - (100 / (1 + rs))
            return rsi.fillna(50)
        except:
            return pd.Series([50] * len(prices), index=prices.index)

    def _calculate_macd(self, prices: pd.Series, fast: int = 12, slow: int = 26) -> pd.Series:
        """Calculate MACD indicator."""
        try:
            ema_fast = prices.ewm(span=fast).mean()
            ema_slow = prices.ewm(span=slow).mean()
            macd = ema_fast - ema_slow
            return macd.fillna(0)
        except:
            return pd.Series([0] * len(prices), index=prices.index)

    def _calculate_pvt(self, data: pd.DataFrame) -> pd.Series:
        """Calculate Price Volume Trend indicator."""
        try:
            price_change = data['close'].pct_change()
            pvt = (price_change * data['volume']).cumsum()
            return pvt.fillna(0)
        except:
            return pd.Series([0] * len(data), index=data.index)

    def _generate_signal(self, calculation_result: Dict[str, Any]) -> SignalType:
        """
        Generate trading signal based on Fibonacci extension analysis.
        
        Args:
            calculation_result: Result from calculate_raw method
            
        Returns:
            SignalType indicating the trading recommendation
        """
        try:
            if not calculation_result or 'current_analysis' not in calculation_result:
                return SignalType.NEUTRAL
            
            current_analysis = calculation_result['current_analysis']
            quality_metrics = calculation_result.get('quality_metrics', {})
            projections = calculation_result.get('projections', [])
            
            # Check overall quality
            overall_quality = quality_metrics.get('overall_quality', 0.0)
            if overall_quality < 0.4:
                return SignalType.NEUTRAL
            
            # Analyze market phase
            market_phase = current_analysis.get('market_phase', 'unknown')
            projection_status = current_analysis.get('projection_status', {})
            
            # Signal generation logic
            if market_phase in ['bullish_extension']:
                avg_confidence = projection_status.get('avg_bullish_confidence', 0.0)
                if avg_confidence > 0.7 and overall_quality > 0.6:
                    return SignalType.STRONG_BUY
                elif avg_confidence > 0.5:
                    return SignalType.BUY
            
            elif market_phase in ['bearish_extension']:
                avg_confidence = projection_status.get('avg_bearish_confidence', 0.0)
                if avg_confidence > 0.7 and overall_quality > 0.6:
                    return SignalType.STRONG_SELL
                elif avg_confidence > 0.5:
                    return SignalType.SELL
            
            # Check proximity to confluence zones
            confluence_proximity = current_analysis.get('confluence_proximity', [])
            for zone in confluence_proximity:
                if zone['distance_pct'] < 2.0 and zone['strength'] > 0.6:
                    if zone['type'] == 'support':
                        return SignalType.BUY
                    elif zone['type'] == 'resistance':
                        return SignalType.SELL
            
            # Check nearest extensions for reversal signals
            nearest_extensions = current_analysis.get('nearest_extensions', [])
            if nearest_extensions:
                nearest = nearest_extensions[0]
                if nearest['distance_pct'] < 1.0 and nearest['probability'] > 0.7:
                    # Near strong extension level - potential reversal
                    if nearest['price'] > current_analysis['current_price']:
                        return SignalType.SELL  # Approaching resistance
                    else:
                        return SignalType.BUY   # Approaching support
            
            return SignalType.HOLD
            
        except Exception as e:
            logger.error(f"Signal generation failed: {str(e)}")
            return SignalType.NEUTRAL

    def get_indicator_config(self) -> Dict[str, Any]:
        """Return the current configuration of the indicator."""
        return {
            'indicator_name': 'FibonacciExtension',
            'version': '1.0.0',
            'lookback_period': self.lookback_period,
            'min_swing_strength': self.min_swing_strength,
            'extension_ratios': self.extension_ratios,
            'volatility_adjustment': self.volatility_adjustment,
            'ml_validation': self.ml_validation,
            'confluence_threshold': self.confluence_threshold,
            'min_projection_distance': self.min_projection_distance,
            'parameters': {
                'type': 'fibonacci_extension',
                'projection_method': 'abc_pattern',
                'validation_method': 'multi_factor',
                'machine_learning': self.ml_validation
            }
        }